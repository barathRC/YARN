
1) Hadoop V2 provides improvement over V1s MapReduce with YARN Yet Another Resource Negotiator.

2) YARN supports distributed processing frameworks such as MapRed, Impala, Spark, etc.

3) YARN Daemons:-

Resource Manager (RM) - Not alike but similar to JT 
One per cluster
Controls App start-up
Schedules resources on the slave nodes
Runs a resource schedular which determines how resources are allocated

Node Manager (NM) - Not alike but similar to TT
One per slave node
Starts all orocess for running the App
Manages resources on the slave nodes
Register it to the RM and provides details on availabel resources.
Monitors resource usage and kills task over consuming resources.
Stores app logs to HDFS.

Job History Server -
One per cluster
Archival of job log files.

CONFIG:-
<name>yarn.resourcemanager.hostname - <value>HADOOPM1:8032 Server name for RM
<name>yarn.nodemanager.local-dirs - <value>/disks/yarn/nm Local files and itermed data and distributed cache are stored
<name>yarn.nodemanager.aux-services - <value>mapreduce_shuffle one ore more aux services that support app framework running under YARN

<name>yarn.log-aggregartion.enable - Enables for log aggregations.
<name>yarn.nodemanager.remote-app-log-dir - HDFS loc to which Node Managers should aggregare their logs to
<name>yarn.nodemanager.log-dirs - Local directory where NM stores local logs.
yarn.log-aggregation.retain-seconds - Time dur to wait before deleting aggregated log files. Disbaled by -1.
These control YARN log aggregation

mapred-site.xml 
<name>mapred.framework.name <value>YARN default is local. Framework
<name>yarn.app.mapreduce.am.staging-dir <value>/user HDFS dir where user jar files are stored. these are needed by app, clients and AM. 
<name>mapred.jobhistory.address <value>HADOOPM1:10020 Address and port used for cluster internal communication with JHServer. 

4) RM and Job History tracker resides in NameNode. 
5) In large clusters, The RM and JHT might be assigned a single machine that acts as a support machine with other daemons.
6) NM runs on each slave datanodes.
7) When an App is submitted to the YARN cluster, the RM allocates the containers as per the config mentioned in the submitted app.
   Our app knows how to submit the job to YARN clutser RM using the yarn-site.xml config file (yarn.resourcemanager.hostname CONFIG IN IT).
8) RM spins a container in one of the slave DN which has Node Manager in it and asks the NM to start AM.
9) Application master is spun up in one of the Slave Node DN having Node Manager which runs as a JVM process.
10) AM is one per cluster and run in its own container.
11) AM is responsible for requesting additional containers from RM.
12) AM then starts in the same container and runs for the wholelife span of the app/ job.
13) AM registers itself with RM. Sends periodic heartbeats to the RM. AM knows where the data blocks resides in HDFS.
14) Containers allocate CPU and Mem on a slave node. Containers actually run the task.
15) After the containers are allocated at specific DN the AM asks the specific NMs in the DNs to start the tasks (Map tasks) in the same containers 
that was requested.
The num of Map tasks are equal to the num of data splits/ data blocks.
16) Once the MAP tasks are completed the intermediate results are persisted locally. The containers are terminated 
and the resources are released to the RM.
16) The AM then requests for RM to provision containers for Reduce tasks. 
17) Containers for Reducer tasks can be spun up anywhere in the cluster since the Map task persisted data is local to the node 
where the data block was located.
18) This involves local persisted intermediate data from Map tasks to travel via the network to the reducer task container. 
this is shuffle and cant be avoided.
19) Data is written into HDFS as part-r-000000 files.
20) Once, reduce tasks are completed, AM notifes JHT server abt the completion. The client is now notified abt the job completion.


LOGS:-
YARN aggregates local logs from NodeManagers to a central HDFS location that is accessible by all the nodes.
Logs are ordered by application/job
Web UI or CLI can be used to view logs. 
Log config is disabled by default.



